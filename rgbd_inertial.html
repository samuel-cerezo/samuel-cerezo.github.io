<!DOCTYPE HTML>

<html>
	<head>
		<title>Samuel Cerezo</title>
		
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>

		<!--------------------------------------------- LaTex Scripts --------------------------------------------------------->
		<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
		<script>
		MathJax = {
		  tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
		};
		</script>
		<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
		<!--------------------------------------------- LaTex Scripts --------------------------------------------------------->

	</head>

	<body class="is-preload">

		<div id="page-wrapper">

			<!-- Header -->
				<header id="header">
					<h1 id="logo"><a href="index.html">Home</a></h1>
					<nav id="nav">
						<ul>
							<li>
								<a href="#">Projects</a>
								<ul>
									<li><a href="rgbd_inertial.html">[2023] Camera Motion Estimation</a></li>
									<li><a href="compressive.html">[2020] Compressive Sensing System</a></li>	
								</ul>
							</li>
							<li><a href="contact.html">Contact me</a></li>
							<li><a href="cv.html">My CV</a></li>
						</ul>
					</nav>
				</header>

			<!-- Main -->
		
				<div id="main" class="wrapper style1">
					<div class="container">
						<header class="major">
							<h1>Camera Motion Estimation from RGB-D-Inertial Scene Flow</h1>
							            <div class="is-size-5 publication-authors">
							              <span class="author-block">
							                <a href="https://scholar.google.com/citations?user=9zhUnGkAAAAJ&hl=en">Samuel Cerezo</a><sup>1</sup>,</span>
							              <span class="author-block">
							                <a href="https://scholar.google.com/citations?user=j_sMzokAAAAJ&hl=en">Javier Civera</a><sup>1</sup>,</span>
							              </span>
							            </div>
							            <div class="is-size-5 publication-authors">
							              <span class="author-block"><sup>1</sup>Universidad de Zaragoza</span>
							            </div>
						
							<p> CVPRW 2024 17-21 June, Seattle, WA</p>
							        <div class="publication-links">
							                <!-- PDF Link. -->
							                <span class="link-block">
							                  <a href="https://arxiv.org/abs/2404.17251" class="external-link button is-normal is-rounded is-dark">
							                    <span class="icon">
							                      <svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><!-- <i class="fas fa-file-pdf"></i> Font Awesome fontawesome.com -->
							                    </span>
							                    <span>Paper</span>
							                  </a>
							                </span>
							                <span class="link-block">
							                  <a href="https://arxiv.org/abs/2404.17251" class="external-link button is-normal is-rounded is-dark">
							                    <span class="icon">
							                      <i class="ai ai-arxiv"></i>
							                    </span>
							                    <span>arXiv</span>
							                  </a>
							                </span>
							                <!-- Code Link. -->
							                <span class="link-block">
							                  <a href="https://github.com/samuel-cerezo/camera_motion_estimation" class="external-link button is-normal is-rounded is-dark">
							                    <span class="icon">
							                      <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
							                    </span>
							                    <span>Code</span>
							                  </a>
							                </span>
								</div>

						</header>

							<div class="col-8 col-12-medium imp-medium">

								<!-- Content -->
									<section id="content">
										
										<h3>Abstract</h3>
										<p>
											In this paper, we introduce a novel formulation for camera motion estimation that integrates RGB-D images and inertial data through scene flow. Our goal is to accurately estimate the camera motion in a rigid 3D environment, along
											with the state of the inertial measurement unit (IMU). Our
											proposed method offers the flexibility to operate as a multiframe optimization or to marginalize older data, thus effectively utilizing past measurements. To assess the performance of our method, we conducted evaluations using both
											synthetic data from the ICL-NUIM dataset and real data
											sequences from the OpenLORIS-Scene dataset. Our results
											show that the fusion of these two sensors enhances the accuracy of camera motion estimation when compared to using
											only visual data.
										</p>
										
										<div align="center"><a href="#" class="image"><img src="images/motion-representation-3D.png" style="width: 40vw"  /></a></div>
										<p>Figure. Motion estimation in an office scene from the ICL-NUIM dataset in two different times t1 and t2. (a) 3D representation of the
											scene. (b) Motion estimation of the objects in the scene. Every velocity is represented by a red arrow on each point. (c) Zoomed-in areas.</p>
										
										<h3>Optimization problem</h3>

										<p>In order to estimate the motion field we formulate an optimization problem over the state $\mathbf{x}$ for which the camera velocity consistency is imposed as well as those terms corresponding to the pre-integration of the IMU readings.
										The joint optimization problem will consist on minimizing a cost function $J(\mathbf{x})$ which is the summation of terms associated to the inertial measurements $J_{i}$ as well as to the camera measurements $J_{c}$.
										Our state estimate $\hat{\mathbf{x}}$ will be the one that minimizing the cost function $J(\mathbf{x})$.</p>
										\begin{align}
												\hat{\mathbf{x}} = min_{\mathbf{x}}  J(\mathbf{x}) = min_{\mathbf{x}}  \left(  J_{c}(\mathbf{x}) + J_{i}(\mathbf{x}) \right)
										\end{align}
										<p>
										As we add more frames, the result is a sliding window of $N$ frames moving along the camera trajectory.
										In the general case, the cost function $J(\mathbf{x})$ can be expressed compactly using as follows:
												\begin{equation}
														J(\mathbf{x})= 
															\sum_{p=i}^{i+N-1}\left(\mathbf{r}_{c_{p}}^\top{\boldsymbol\Sigma}_{c_{p}}^{-1}\mathbf{r}_{c_{p}} +  \mathbf{r}_{\Delta \mathbf{v}_{p}}^\top{\boldsymbol\Sigma}_{\Delta \mathbf{v}_{p}}^{-1} \mathbf{r}_{\Delta \mathbf{v}_{p}}\right)
															+ \mathbf{r}_{bg}^\top{\boldsymbol\Sigma}_{\boldsymbol\omega}^{-1} \mathbf{r}_{bg}
															+ \mathbf{r}_{ba}^\top{\boldsymbol\Sigma}_{\mathbf{a}}^{-1} \mathbf{r}_{ba}   
															+ \sum_{l=i}^{i+N}\mathbf{r}_{\omega_l }^\top{\boldsymbol\Sigma}_{\boldsymbol\omega}^{-1}\mathbf{r}_{\omega_l}
												\end{equation}
										and the state $\mathbf{x} \in \mathbb{R}^{6N+8}$ is defined as:
										\begin{equation}
												\mathbf{x} = \left[ {\mathbf{v}}_i^\top,{\boldsymbol\omega}_i^\top,\dots,{\mathbf{v}}_{i+N-1}^\top,{\boldsymbol\omega}_{i+N-1}^\top,{\mathbf{g}}^\top,{\mathbf{b}^g}^\top, {\mathbf{b}^a}^\top \right]^\top
										\end{equation}
										</p>

										<h3>Marginalization</h3>
										<p>Consider the case in following figure, using a 3-frames-sliding-window. When the $l$-frame comes, the optimization is done. 
										However, in order to keep a 3-frame window, when the next frame $l+1$ comes, we need to marginalize out $\mathbf{v}_i$ and $\boldsymbol{\omega}_i$.</p>
										<div align="center"><a href="#" class="image"><img src="images/grafo_marginalizacion.png" style="width: 40vw"  /></a></div>
										<p>
										The Hessian $\mathbf{H}$ contains the second derivatives of the cost function with respect to the state variables, that encodes how every state variable affects the others. 
										We denote as $\alpha$ the block of variables we would like to marginalize, and $\beta$ the block of variables we would like to keep.
										When marginalizing a set $\alpha$ of variables, we gather all factors dependent on them as well as the connected variables $\beta$. This is done by means of the $\textit{Schur Complement}$, which is defined as follows. 
											\begin{equation}
													\mathbf{H}^* = \mathbf{H}_{\beta\beta} 
																 - \mathbf{H}_{\alpha\beta}^\top\mathbf{H}_{\alpha\alpha}^{-1}\mathbf{H}_{\alpha\beta}   
											\end{equation}
										</p>

										<h3>Experiments</h3>
										<p>
											We chose to evaluate our proposal on an extended version of the living room sequences in the ICL-NUIM dataset. 
											ICL-NUIM is a synthetic photorealistic dataset that provides ground truth poses as well as 3D scene models to benchmark reconstruction and/or localization approaches. 
											As ICL-NUIM does not provide IMU data, we fit splines to the ground truth poses to simulate continuous trajectories and simulated IMU measurements from them. 
											We also evaluated our RGB-D-inertial flow in the OpenLORIS-Scene datasets, in which data are collected in real-world indoor scenes, 
											for multiple times in each place to include natural scene changes in everyday scenarios. 
											RGB-D images and IMU measurements from a RealSense D435i are provided. 
											The ground truth trajectory was recorded by an OptiTrack MCS, that tracked artificial markers deployed on the Segway robot used to record the data.
										</p>
										<!---- Additional options: 
										<ul>
											<li>1)</li>
											<li>2)</li>
											<li>3)</li>
										</ul>
										-->
										<h3>Conclusions</h3>
										
										<p>
										In this work we present a novel camera motion estimation based on RGB-D-I scene flow. Specifically, we formulate the fusion of RGB-D and inertial data as a joint optimization using scene flow residuals and pre-integrated IMU residuals, weighted by their corresponding covariances. We also consider the marginalization of old states in order to keep a compact optimization. 
										We evaluated our approach on a synthetic dataset, ICL-NUIM, and on a real dataset, OpenLORIS, both publicly available. Our results quantify the improvement that inertial fusion can offer to RGB-D scene flow techniques. We evaluated our approach on a synthetic dataset, ICL-NUIM, and on a real dataset, OpenLORIS, both publicly available. Our results quantify the improvement that inertial fusion can offer to RGB-D scene flow. 
										</p>

										<h1>Article</h1>
										<p> The article is available on IEEE Xplore (link below).</p>
										<p><a href="https://ieeexplore.ieee.org/author/864705730663272" target="_blank"><span class="label">Click here</span></a></p>
									
									</section>

							</div>

							<div class="row gtr-150">
								<div class="col-6 col-12-medium">
	
								<!-- Sidebar -->
									<section id="sidebar">

										<hr />
										<section>
											<h3>Other projects: Compressive Sensing System</h3>
											<p>...</p>
											
											<a href="compressive.html" class="image fit"><img src="images/primer_resultado.jpg" alt="" /></a>
											<footer>
												<ul class="actions">
													<li><a href="compressive.html" class="button">Learn More</a></li>
												</ul>
											</footer>
											<hr />
										</section>
									</section>
							</div>
						</div>
					</div>
				</div>

			<!-- Footer -->
				<footer id="footer">
					<ul class="icons">
						<li><a href="https://twitter.com/SamuelCerezoo/" target="_blank" class="icon brands alt fa-twitter"><span class="label">Twitter</span></a></li>
						<li><a href="https://www.linkedin.com/in/samuel-cerezo/" target="_blank" class="icon brands alt fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
						<li><a href="https://www.instagram.com/samucerezo/" target="_blank" class="icon brands alt fa-instagram"><span class="label">Instagram</span></a></li>
						<li><a href="https://github.com/samuelcerezoo/" target="_blank" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
						<li><a href="mailto: samueladriancerezo@gmail.com" class="icon solid alt fa-envelope"><span class="label">Email</span></a></li>
					</ul>
					<ul class="copyright">
						<li>&copy; Untitled. All rights reserved.</li><li>Design: HTML5 UP</a></li>
					</ul>
				</footer>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
