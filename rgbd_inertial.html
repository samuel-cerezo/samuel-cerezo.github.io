<!DOCTYPE HTML>

<html>
	<head>
		<title>Samuel Cerezo</title>
		
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>

		<!--------------------------------------------- LaTex Scripts --------------------------------------------------------->
		<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
		<script>
		MathJax = {
		  tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
		};
		</script>
		<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
		<!--------------------------------------------- LaTex Scripts --------------------------------------------------------->

	</head>

	<body class="is-preload">

		<div id="page-wrapper">

			<!-- Header -->
				<header id="header">
					<h1 id="logo"><a href="index.html">Home</a></h1>
					<nav id="nav">
						<ul>
							<li>
								<a href="#">Projects</a>
								<ul>
									<li><a href="SLAM&Render.html">[2025] SLAM&Render</a></li>
									<li><a href="rgbd_inertial.html">[2023] Camera Motion Estimation</a></li>
									<li><a href="compressive.html">[2020] Compressive Sensing System</a></li>	
								</ul>
							</li>
							<li><a href="contact.html">Contact me</a></li>
							<li><a href="cv.html">My CV</a></li>
						</ul>
					</nav>
				</header>

			<!-- Main -->
		
				<div id="main" class="wrapper style1">
					<div class="container">
						<header class="major">
							<h2>Camera Motion Estimation from RGB-D-Inertial Scene Flow</h2>
							            <h1 class="is-size-5 publication-authors">
							              <span class="author-block">
							                <a href="https://scholar.google.com/citations?user=9zhUnGkAAAAJ&hl=en">Samuel Cerezo</a><sup>1</sup>,</span>
							              <span class="author-block">
							                <a href="https://scholar.google.com/citations?user=j_sMzokAAAAJ&hl=en">Javier Civera</a><sup>1</sup></span>
							              </span>
							            </h1>
							            <h1 class="is-size-5 publication-authors">
							              <span class="author-block"><sup>1</sup>Universidad de Zaragoza</span>
							            </h1>
						
							<h0> 2024 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</h1> <p></p>
							        <div class="publication-links">
							                <span class="link-block">
							                  <a href="https://ieeexplore.ieee.org/document/10677884" class="external-link button is-normal is-rounded is-dark">
							                    <span>Paper</span>
							                  </a>
							                </span>
							                <span class="link-block">
							                  <a href="https://arxiv.org/abs/2404.17251" class="external-link button is-normal is-rounded is-dark">
							                    <span>arXiv</span>
							                  </a>
							                </span>
							                <span class="link-block">
							                  <a href="https://github.com/samuel-cerezo/camera_motion_estimation" class="external-link button is-normal is-rounded is-dark">
							                    <span>Code</span>
							                  </a>
							                </span>
								</div>

						</header>

							<div class="col-8 col-12-medium imp-medium">

								<!-- Content -->
									<section id="content">
										
										<h3>Abstract</h3>
										<p>
											In this paper, we introduce a novel formulation for camera motion estimation that integrates RGB-D images and inertial data through scene flow. Our goal is to accurately estimate the camera motion in a rigid 3D environment, along
											with the state of the inertial measurement unit (IMU). Our
											proposed method offers the flexibility to operate as a multiframe optimization or to marginalize older data, thus effectively utilizing past measurements. To assess the performance of our method, we conducted evaluations using both
											synthetic data from the ICL-NUIM dataset and real data
											sequences from the OpenLORIS-Scene dataset. Our results
											show that the fusion of these two sensors enhances the accuracy of camera motion estimation when compared to using
											only visual data.
										</p>
										
										<div align="center"><a href="#" class="image"><img src="images/motion-representation-3D.png" style="width: 40vw"  /></a></div>
										<p>Figure. Motion estimation in an office scene from the ICL-NUIM dataset in two different times t1 and t2. (a) 3D representation of the
											scene. (b) Motion estimation of the objects in the scene. Every velocity is represented by a red arrow on each point. (c) Zoomed-in areas.</p>
										
										<h3>Optimization problem</h3>

										<p>In order to estimate the motion field we formulate an optimization problem over the state $\mathbf{x}$ for which the camera velocity consistency is imposed as well as those terms corresponding to the pre-integration of the IMU readings.
										The joint optimization problem will consist on minimizing a cost function $J(\mathbf{x})$ which is the summation of terms associated to the inertial measurements $J_{i}$ as well as to the camera measurements $J_{c}$.
										Our state estimate $\hat{\mathbf{x}}$ will be the one that minimizing the cost function $J(\mathbf{x})$.</p>
										\begin{align}
												\hat{\mathbf{x}} = min_{\mathbf{x}}  J(\mathbf{x}) = min_{\mathbf{x}}  \left(  J_{c}(\mathbf{x}) + J_{i}(\mathbf{x}) \right)
										\end{align}
										<p>
										As we add more frames, the result is a sliding window of $N$ frames moving along the camera trajectory.
										In the general case, the cost function $J(\mathbf{x})$ can be expressed compactly using as follows:
												\begin{equation}
														J(\mathbf{x})= 
															\sum_{p=i}^{i+N-1}\left(\mathbf{r}_{c_{p}}^\top{\boldsymbol\Sigma}_{c_{p}}^{-1}\mathbf{r}_{c_{p}} +  \mathbf{r}_{\Delta \mathbf{v}_{p}}^\top{\boldsymbol\Sigma}_{\Delta \mathbf{v}_{p}}^{-1} \mathbf{r}_{\Delta \mathbf{v}_{p}}\right)
															+ \mathbf{r}_{bg}^\top{\boldsymbol\Sigma}_{\boldsymbol\omega}^{-1} \mathbf{r}_{bg}
															+ \mathbf{r}_{ba}^\top{\boldsymbol\Sigma}_{\mathbf{a}}^{-1} \mathbf{r}_{ba}   
															+ \sum_{l=i}^{i+N}\mathbf{r}_{\omega_l }^\top{\boldsymbol\Sigma}_{\boldsymbol\omega}^{-1}\mathbf{r}_{\omega_l}
												\end{equation}
										and the state $\mathbf{x} \in \mathbb{R}^{6N+8}$ is defined as:
										\begin{equation}
												\mathbf{x} = \left[ {\mathbf{v}}_i^\top,{\boldsymbol\omega}_i^\top,\dots,{\mathbf{v}}_{i+N-1}^\top,{\boldsymbol\omega}_{i+N-1}^\top,{\mathbf{g}}^\top,{\mathbf{b}^g}^\top, {\mathbf{b}^a}^\top \right]^\top
										\end{equation}
										</p>

										<h3>Marginalization</h3>
										<p>Consider the case in following figure, using a 3-frames-sliding-window. When the $l$-frame comes, the optimization is done. 
										However, in order to keep a 3-frame window, when the next frame $l+1$ comes, we need to marginalize out $\mathbf{v}_i$ and $\boldsymbol{\omega}_i$.</p>
										<div align="center"><a href="#" class="image"><img src="images/grafo_marginalizacion.png" style="width: 40vw"  /></a></div>
										<p>
										The Hessian $\mathbf{H}$ contains the second derivatives of the cost function with respect to the state variables, that encodes how every state variable affects the others. 
										We denote as $\alpha$ the block of variables we would like to marginalize, and $\beta$ the block of variables we would like to keep.
										When marginalizing a set $\alpha$ of variables, we gather all factors dependent on them as well as the connected variables $\beta$. This is done by means of the $\textit{Schur Complement}$, which is defined as follows. 
											\begin{equation}
													\mathbf{H}^* = \mathbf{H}_{\beta\beta} 
																 - \mathbf{H}_{\alpha\beta}^\top\mathbf{H}_{\alpha\alpha}^{-1}\mathbf{H}_{\alpha\beta}   
											\end{equation}
										</p>

										<h3>Experiments</h3>
										<p>
											We chose to evaluate our proposal on an extended version of the living room sequences in the ICL-NUIM dataset. 
											ICL-NUIM is a synthetic photorealistic dataset that provides ground truth poses as well as 3D scene models to benchmark reconstruction and/or localization approaches. 
											As ICL-NUIM does not provide IMU data, we fit splines to the ground truth poses to simulate continuous trajectories and simulated IMU measurements from them. 
											We also evaluated our RGB-D-inertial flow in the OpenLORIS-Scene datasets, in which data are collected in real-world indoor scenes, 
											for multiple times in each place to include natural scene changes in everyday scenarios. 
											RGB-D images and IMU measurements from a RealSense D435i are provided. 
											The ground truth trajectory was recorded by an OptiTrack MCS, that tracked artificial markers deployed on the Segway robot used to record the data.
										</p>
										<!---- Additional options: 
										<ul>
											<li>1)</li>
											<li>2)</li>
											<li>3)</li>
										</ul>
										-->
										<h3>Conclusions</h3>
										
										<p>
										In this work we present a novel camera motion estimation based on RGB-D-I scene flow. Specifically, we formulate the fusion of RGB-D and inertial data as a joint optimization using scene flow residuals and pre-integrated IMU residuals, weighted by their corresponding covariances. We also consider the marginalization of old states in order to keep a compact optimization. 
										We evaluated our approach on a synthetic dataset, ICL-NUIM, and on a real dataset, OpenLORIS, both publicly available. Our results quantify the improvement that inertial fusion can offer to RGB-D scene flow techniques. We evaluated our approach on a synthetic dataset, ICL-NUIM, and on a real dataset, OpenLORIS, both publicly available. Our results quantify the improvement that inertial fusion can offer to RGB-D scene flow. 
										</p>
				
									</section>

							</div>

							<div class="row gtr-150">
								<div class="col-6 col-12-medium">
	
								<!-- Sidebar -->
									<section id="sidebar">

										<hr />
										<section>
											<h3>Other projects: Compressive Sensing System</h3>
											<p>...</p>
											
											<a href="compressive.html" class="image fit"><img src="images/primer_resultado.jpg" alt="" /></a>
											<footer>
												<ul class="actions">
													<li><a href="compressive.html" class="button">Learn More</a></li>
												</ul>
											</footer>
											<hr />
										</section>
									</section>
							</div>
						</div>
					</div>
				</div>

			<!-- Footer -->
			<footer id="footer">
				<ul class="icons">
					<li><a href="https://twitter.com/SamuelCerezoo/" target="_blank" class="icon brands alt fa-twitter"><span class="label">Twitter</span></a></li>
					<li><a href="https://www.linkedin.com/in/samuel-cerezo/" target="_blank" class="icon brands alt fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
					<li><a href="https://www.instagram.com/samucerezo/" target="_blank" class="icon brands alt fa-instagram"><span class="label">Instagram</span></a></li>
					<li><a href="https://github.com/samuel-cerezo/" target="_blank" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
					<li><a href="mailto: samueladriancerezo@gmail.com" class="icon solid alt fa-envelope"><span class="label">Email</span></a></li>
				</ul>
				<ul class="copyright">
					<li>&copy; Samuel Cerezo. All rights reserved.</li><li></a></li>
				</ul>
			</footer>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
